<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Shiqi Liu


  | Publications

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üçÄ</text></svg>">

<link rel="stylesheet" href="/BLOG/assets/css/main.css">
<link rel="canonical" href="/BLOG/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/BLOG/assets/js/theme.js"></script>
<script src="/BLOG/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="http://localhost:4000/BLOG/">
          <!--       Shiqi Liu -->
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/BLOG/">
              Home
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/BLOG/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/BLOG/publications/">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/BLOG/socialpapers/">
                Social Science
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/BLOG/others/">
                Miscellaneous
                
              </a>
          </li>
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3 abbr">
  
    <img src="/BLOG/assets/teaser/change detection.png" class="teaser img-fluid z-depth-1" />
  
  
  </div>

  <div id="liu2021auto" class="col-sm-9">
    
      <div class="title">Auto robust relative radiometric normalization via latent change noise modelling</div>
      <div class="author">
        
          
          
          
          
            
              
            
          
          
          
            
              
                Shiqi Liu,
              
            
          
        
          
          
          
          
          
          
            
              
                
                    Lu Wang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                    Jie Lian,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                    Cong Liu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                    Xuchen Zhan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                    Jintao Lu,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                    Jie Liu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                    Ting Wang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                    Dong Geng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                    Hongwei Duan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                and  others
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2111.12406</em>
      
      
      
      , 2021
      
      </div>
    

    <div class="links">
    
      
      <a href="https://arxiv.org/abs/2111.12406" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
    
    
    
    </div>
        <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Relative radiometric normalization(RRN) of different satellite images of the same terrain is necessary for change detection, object classification/segmentation, and map-making tasks. However, traditional RRN models are not robust, disturbing by object change, and RRN models precisely considering object change can not robustly obtain the no-change set. This paper proposes auto robust relative radiometric normalization methods via latent change noise modeling. They utilize the prior knowledge that no change points possess small-scale noise under relative radiometric normalization and that change points possess large-scale radiometric noise after radiometric normalization, combining the stochastic expectation maximization method to quickly and robustly extract the no-change set to learn the relative radiometric normalization mapping functions. This makes our model theoretically grounded regarding the probabilistic theory and mathematics deduction. Specifically, when we select histogram matching as the relative radiometric normalization learning scheme integrating with the mixture of Gaussian noise(HM-RRN-MoG), the HM-RRN-MoG model achieves the best performance. Our model possesses the ability to robustly against clouds/fogs/changes. Our method naturally generates a robust evaluation indicator for RRN that is the no-change set root mean square error. We apply the HM-RRN-MoG model to the latter vegetation/water change detection task, which reduces the radiometric contrast and NDVI/NDWI differences on the no-change set, generates consistent and comparable results. We utilize the no-change set into the building change detection task, efficiently reducing the pseudo-change and boosting the precision.</p>
    </div>
    
        <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@articleliu2021auto</span><span class="err">,</span>
  <span class="err">title=Auto</span> <span class="err">robust</span> <span class="err">relative</span> <span class="err">radiometric</span> <span class="err">normalization</span> <span class="err">via</span> <span class="err">latent</span> <span class="err">change</span> <span class="err">noise</span> <span class="err">modelling,</span>
  <span class="err">author=Liu,</span> <span class="err">Shiqi</span> <span class="err">and</span> <span class="err">Wang,</span> <span class="err">Lu</span> <span class="err">and</span> <span class="err">Lian,</span> <span class="err">Jie</span> <span class="err">and</span> <span class="err">Liu,</span> <span class="err">Cong</span> <span class="err">and</span> <span class="err">Zhan,</span> <span class="err">Xuchen</span> <span class="err">and</span> <span class="err">Lu,</span> <span class="err">Jintao</span> <span class="err">and</span> <span class="err">Liu,</span> <span class="err">Jie</span> <span class="err">and</span> <span class="err">Wang,</span> <span class="err">Ting</span> <span class="err">and</span> <span class="err">Geng,</span> <span class="err">Dong</span> <span class="err">and</span> <span class="err">Duan,</span> <span class="err">Hongwei</span> <span class="err">and</span> <span class="err">others,</span>
  <span class="err">journal=arXiv</span> <span class="err">preprint</span> <span class="err">arXiv:2111.12406,</span>
  <span class="err">year=2021</span></code></pre></figure>
    </div>
    
  </div>



</div>
</li>
<li><div class="row">
  <div class="col-sm-3 abbr">
  
    <img src="/BLOG/assets/teaser/poisson.png" class="teaser img-fluid z-depth-1" />
  
  
  </div>

  <div id="liu2021automatically" class="col-sm-9">
    
      <div class="title">Automatically eliminating seam lines with Poisson editing in complex relative radiometric normalization mosaicking scenarios</div>
      <div class="author">
        
          
          
          
          
            
              
            
          
          
          
            
              
                Shiqi Liu,
              
            
          
        
          
          
          
          
          
          
            
              
                
                    Jie Lian,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                    Xuchen Zhan,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                    Cong Liu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                    Yuze Tian,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                and Hongwei Duan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2106.07441</em>
      
      
      
      , 2021
      
      </div>
    

    <div class="links">
    
      
      <a href="https://arxiv.org/abs/2106.07441" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
    
    
    
    </div>
        <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p> Relative radiometric normalization (RRN) mosaicking among multiple remote sensing images is crucial for the downstream tasks, including map-making, image recognition, semantic segmentation, and change detection. However, there are often seam lines on the mosaic boundary and radiometric contrast left, especially in complex scenarios, making the appearance of mosaic images unsightly and reducing the accuracy of the latter classification/recognition algorithms. This paper renders a novel automatical approach to eliminate seam lines in complex RRN mosaicking scenarios. It utilizes the histogram matching on the overlap area to alleviate radiometric contrast, Poisson editing to remove the seam lines, and merging procedure to determine the normalization transfer order. Our method can handle the mosaicking seam lines with arbitrary shapes and images with extreme topological relationships (with a small intersection area). These conditions make the main feathering or blending methods, e.g., linear weighted blending and Laplacian pyramid blending, unavailable. In the experiment, our approach visually surpasses the automatic methods without Poisson editing and the manual blurring and feathering method using GIMP software.</p>
    </div>
    
        <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@articleliu2021automatically</span><span class="err">,</span>
  <span class="err">title=Automatically</span> <span class="err">eliminating</span> <span class="err">seam</span> <span class="err">lines</span> <span class="err">with</span> <span class="err">Poisson</span> <span class="err">editing</span> <span class="err">in</span> <span class="err">complex</span> <span class="err">relative</span> <span class="err">radiometric</span> <span class="err">normalization</span> <span class="err">mosaicking</span> <span class="err">scenarios,</span>
  <span class="err">author=Liu,</span> <span class="err">Shiqi</span> <span class="err">and</span> <span class="err">Lian,</span> <span class="err">Jie</span> <span class="err">and</span> <span class="err">Zhan,</span> <span class="err">Xuchen</span> <span class="err">and</span> <span class="err">Liu,</span> <span class="err">Cong</span> <span class="err">and</span> <span class="err">Tian,</span> <span class="err">Yuze</span> <span class="err">and</span> <span class="err">Duan,</span> <span class="err">Hongwei,</span>
  <span class="err">journal=arXiv</span> <span class="err">preprint</span> <span class="err">arXiv:2106.07441,</span>
  <span class="err">year=2021</span></code></pre></figure>
    </div>
    
  </div>



</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3 abbr">
  
    <img src="/BLOG/assets/teaser/mastersurvey.jpg" class="teaser img-fluid z-depth-1" />
  
  
        
            <abbr class="badge">Master</abbr>
        
  
  </div>

  <div id="liu2020Survey" class="col-sm-9">
    
      <div class="title">Ê∑±Â∫¶Ë°®Á§∫Â≠¶‰π†Âú®Êó∂Â∫è„ÄÅÂõæÂÉè‰∏éËßÜÈ¢ëÊï∞ÊçÆ‰∏äÁöÑÂ∫îÁî®ÂàùÊé¢(Primary Applications of Deep Representation Learning in Time Series, Image and Video Data)</div>
      <div class="author">
        
          
          
          
          
            
              
            
          
          
          
            
            Shiqi Liu
            
          
        
      </div>

      <div class="periodical">
      
        <em>Xi‚Äôan jiaotong University Master Thesis</em>
      
      
        (<b>Master</b>)
      
      
      , 2020
      
      </div>
    

    <div class="links">
    
      
      <a href="/BLOG/assets/pdf/master.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
    
    
    
    </div>
        <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The advent of the era of big data has brought about a drastic change in the basic model of machine learning. The traditional method of artificial feature extraction has been unable to meet the actual needs of diversified, complicated, and quantified application goals. Deep learning has gradually become the mainstream of applications due to its end-to-end data-driven adaptive feature extraction, which has attracted widespread attention in academia and industry. Based on the current research status, this article introduces representative deep learning methods for typical data types such as time series, images, and videos. Based on a deep understanding of the characteristics of the corresponding deep learning methods, it is applied to application problems such as hydrological time series processing, image factor learning, and emotional prediction of EEG data. The paper includes the following research contents. 
First of all, we use the deep long short-term memory network (LSTM) to predict the daily runoff data of hydrological stations. Using the long short-term memory characteristics of LSTM, the deep LSTM achieves good results in Yichang Hydrological Station. We further compare the relative systematic error, relative standard deviation and relative error range of prediction results of LSTM and backpropagation neural network(BPNN) in flood season of Yi Chang, and the comparison results verify the superiority of the deep LSTM method over BPNN in predicting daily runoff in flood season. In order to verify the effectiveness of the proposed method, we compared the results of LSTM models and deep LSTM models on the data obtained from Cuntan, Wanxian, Fengjie and Yichang hydrological stations. The results show that both moels are suitable for daily runoff data, and deep LSTM models achieve better prediction results.
The second part is about one of the main models of deep representation learning, the variational autoencoder (VAE). We focus on supervising the factors extracted by VAE. VAE is used to learn the independent low-dimensional representation of images, but it faces the problem that some pre-specified factors are ignored. We assert that the mutual information of the input and each learned factor of the representation plays a necessary indicator to discover the influential factors. At the same time, we delve into the objective function of VAE, which shows that it tends to induce the sparsity of mutual information of factors when it exceeds the essential dimension of the data, resulting in some non-influential factors which can be ignored and which have negligible data reconstruction capabilities. We show that mutual information can affect the lower bound of VAE reconstruction errors and down-stream classification tasks. We propose an algorithm to calculate the mutual information indicator of VAE and prove its consistency. Experimental results on the MNIST, CelebA, and DEAP datasets show that mutual information helps determine influential generating factors. Besides some generation factors are interpretable, which can help down-stream generation and classification tasks. At last, we focus on deep representation learning applied to EEG video data emotion recognition. By characterizing the VAE and LSTM-based EEG emotion representation learning architecture, the method reaches the latest level. At the same time, the model can also monitor the learned feature shapes. 
Summarizing the contents of the above three parts, the deep representation learning achieves good performance in time series, image, and video data and can reach the international frontier in some known application fields, such as hydrological prediction, image factor learning, and EEG emotion recognition. Therefore, this paper has significant research and application inspiration for the methodological expansion of these application fields and the methodological level of deep learning itself.</p>
    </div>
    
        <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@articleliu2020Survey</span><span class="err">,</span>
  <span class="err">title=Ëá™Ê≠•Â≠¶‰π†Á†îÁ©∂ÁªºËø∞</span><span class="p">(</span><span class="nl">A</span> <span class="err">Survey</span> <span class="err">On</span> <span class="err">Self-paced</span> <span class="err">Learning</span><span class="p">)</span><span class="c">,</span>
  <span class="c">author=Liu, Shiqi,</span>
  <span class="c">journal=Xi‚Äôan jiaotong University Bachelor,</span>
  <span class="c">year=2020,</span>
  <span class="c">publisher=</span></code></pre></figure>
    </div>
    
  </div>



</div>
</li>
<li><div class="row">
  <div class="col-sm-3 abbr">
  
    <img src="/BLOG/assets/teaser/VAE_sparsity.png" class="teaser img-fluid z-depth-1" />
  
  
        
            <abbr class="badge">Pattern Recognition</abbr>
        
  
  </div>

  <div id="liu2020discovering" class="col-sm-9">
    
      <div class="title">Discovering influential factors in variational autoencoders</div>
      <div class="author">
        
          
          
          
          
            
              
            
          
          
          
            
              
                Shiqi Liu,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                <a href="https://scholar.google.com/citations?hl=en&amp;user=xrofpMsAAAAJ" target="_blank">Jingxin Liu</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                <a href="https://scholar.google.com/citations?hl=en&amp;user=vM6yGTEAAAAJ" target="_blank">Qian Zhao</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                <a href="https://scholar.google.com/citations?hl=en&amp;user=IePM9RsAAAAJ" target="_blank">Xiangyong Cao</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                    Huibin Li,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                <a href="https://scholar.google.com/citations?hl=en&amp;user=an6w-64AAAAJ" target="_blank">Deyu Meng</a>,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                    Hongying Meng,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                and Sheng Liu
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Pattern Recognition</em>
      
      
        (<b>Pattern Recognition</b>)
      
      
      , 2020
      
      </div>
    

    <div class="links">
    
      
      <a href="https://arxiv.org/pdf/1809.01804.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
      <a href="https://github.com/647LiuSQ/Discovering-influential-factors-in-variational-autoencoders" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>
        <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In the field of machine learning, it is still a critical issue to identify and supervise the learned representation without manually intervening or intuition assistance to extract useful knowledge or serve for the downstream tasks. In this work, we focus on supervising the influential factors extracted by the variational autoencoder (VAE). The VAE is proposed to learn independent low dimension representation while facing the problem that sometimes pre-set factors are ignored. We argue that the mutual information of the input and each learned factor of the representation plays a necessary indicator of discovering the influential factors. We find the VAE objective inclines to induce mutual information sparsity in factor dimension over the data intrinsic dimension and therefore result in some non-influential factors whose function on data reconstruction could be ignored. We show mutual information also influences the lower bound of VAE‚Äôs reconstruction error and downstream classification task. To make such indicator applicable, we design an algorithm for calculating the mutual information for VAE and prove its consistency. Experimental results on MNIST, CelebA and DEAP datasets show that mutual information can help determine influential factors, of which some are interpretable and can be used to further generation and classification tasks, and help discover the variant that connects with emotion on DEAP dataset.</p>
    </div>
    
        <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@articleliu2020Discovering</span><span class="err">,</span>
  <span class="err">title=Discovering</span> <span class="err">influential</span> <span class="err">factors</span> <span class="err">in</span> <span class="err">variational</span> <span class="err">autoencoders,</span>
  <span class="err">author=Liu,</span> <span class="err">Shiqi</span> <span class="err">and</span> <span class="err">Liu,</span> <span class="err">Jingxin</span> <span class="err">and</span> <span class="err">Zhao,</span> <span class="err">Qian</span> <span class="err">and</span> <span class="err">Cao,</span> <span class="err">Xiangyong</span> <span class="err">and</span> <span class="err">Li,</span> <span class="err">Huibin</span> <span class="err">and</span> <span class="err">Meng,</span> <span class="err">Deyu</span> <span class="err">and</span> <span class="err">Meng,</span> <span class="err">Hongying</span> <span class="err">and</span> <span class="err">Liu,</span> <span class="err">Sheng,</span>
  <span class="err">journal=Pattern</span> <span class="err">Recognition,</span>
  <span class="err">year=2020,</span>
  <span class="err">publisher=Elsevier</span></code></pre></figure>
    </div>
    
  </div>



</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3 abbr">
  
    <img src="/BLOG/assets/teaser/factorsdimension.jpg" class="teaser img-fluid z-depth-1" />
  
  
        
            <abbr class="badge">Openreview</abbr>
        
  
  </div>

  <div id="liu2018preliminary" class="col-sm-9">
    
      <div class="title">Preliminary theoretical troubleshooting in variational autoencoder</div>
      <div class="author">
        
          
          
          
          
            
              
            
          
          
          
            
              
                Shiqi Liu,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                <a href="https://scholar.google.com/citations?hl=en&amp;user=vM6yGTEAAAAJ" target="_blank">Qian Zhao</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                <a href="https://scholar.google.com/citations?hl=en&amp;user=IePM9RsAAAAJ" target="_blank">Xiangyong Cao</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                and <a href="https://scholar.google.com/citations?hl=en&amp;user=an6w-64AAAAJ" target="_blank">Deyu Meng</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Openreview</em>
      
      
        (<b>Openreview</b>)
      
      
      , 2018
      
      </div>
    

    <div class="links">
    
      
      <a href="https://openreview.net/pdf?id=SkERSm-0-" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
      <a href="https://github.com/647LiuSQ/Discovering-influential-factors-in-variational-autoencoders" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>
        <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>What would be learned by variational autoencoder(VAE) and what influence the disentanglement of VAE? This paper tries to preliminarily address VAE‚Äôs intrinsic dimension, real factor, disentanglement and indicator issues theoretically in the idealistic situation and implementation issue practically through noise modeling perspective in the realistic case. On intrinsic dimension issue, due to information conservation, the idealistic VAE learns and only learns intrinsic factor dimension. Besides, suggested by mutual information separation property, the constraint induced by Gaussian prior to the VAE objective encourages the information sparsity in dimension. On disentanglement issue, subsequently, inspired by information conservation theorem the clarification on disentanglement in this paper is made. On real factor issue, due to factor equivalence, the idealistic VAE possibly learns any factor set in the equivalence class. On indicator issue, the behavior of current disentanglement metric is discussed, and several performance indicators regarding the disentanglement and generating influence are subsequently raised to evaluate the performance of VAE model and to supervise the used factors. On implementation issue, the experiments under noise modeling and constraints empirically testify the theoretical analysis and also show their own characteristic in pursuing disentanglement.</p>
    </div>
    
        <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@articleliu2018preliminary</span><span class="err">,</span>
  <span class="err">title=Preliminary</span> <span class="err">theoretical</span> <span class="err">troubleshooting</span> <span class="err">in</span> <span class="err">variational</span> <span class="err">autoencoder,</span>
  <span class="err">author=Liu,</span> <span class="err">Shiqi</span> <span class="err">and</span> <span class="err">Zhao,</span> <span class="err">Qian,</span> <span class="err">and</span> <span class="err">Cao,</span> <span class="err">Xiangyong</span> <span class="err">and</span> <span class="err">Meng,</span> <span class="err">Deyu,</span>
  <span class="err">journal=Openreview,</span>
  <span class="err">year=2018,</span>
  <span class="err">publisher=Openreview</span></code></pre></figure>
    </div>
    
  </div>



</div>
</li>
<li><div class="row">
  <div class="col-sm-3 abbr">
  
    <img src="/BLOG/assets/teaser/concave conjugacy.png" class="teaser img-fluid z-depth-1" />
  
  
        
            <abbr class="badge">CIS</abbr>
        
  
  </div>

  <div id="liu2018understanding" class="col-sm-9">
    
      <div class="title">Understanding self-paced learning under concave conjugacy theory</div>
      <div class="author">
        
          
          
          
          
            
              
            
          
          
          
            
              
                Shiqi Liu,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                <a href="https://scholar.google.com/citations?hl=en&amp;user=OHEfQU0AAAAJ" target="_blank">Zilu Ma</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                <a href="https://scholar.google.com/citations?hl=en&amp;user=an6w-64AAAAJ" target="_blank">Deyu Meng</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                    Kai-Dong Wang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                and Yong Zhang
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Communiations in Information and Systems</em>
      
      
        (<b>CIS</b>)
      
      
      , 2018
      
      </div>
    

    <div class="links">
    
      
      <a href="https://www.intlpress.com/site/pub/files/_fulltext/journals/cis/2018/0018/0001/CIS-2018-0018-0001-a001.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
    
    
      
      <a href="/BLOG/assets/pdf/curriculum-present.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    </div>
        <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>By simulating the easy-to-hard learning manners of humans/animals, the learning regimes called curriculum learning (CL) and selfpaced learning (SPL) have been recently investigated and invoked broad interests. However, the intrinsic mechanism for analyzing why such learning regimes can work has not been comprehensively investigated. To this issue, this paper proposes a concave conjugacy theory for looking into the insight of CL/SPL. Specifically, by using this theory, we prove the equivalence of the SPL regime and a latent concave objective, which is closely related to the known non-convex regularized penalty widely used in statistics and machine learning. Beyond the previous theory for explaining CL/SPL insights, this new theoretical framework on one hand facilitates two direct approaches for designing new SPL models for certain tasks, and on the other hand can help conduct the latent objective of selfpaced curriculum learning, which is the advanced version of both CL/SPL and possess advantages of both learning regimes to a certain extent. This further facilitates a theoretical understanding for SPCL, instead of only CL/SPL as conventional. Under this theory, we attempt to attain intrinsic latent objectives of two curriculum forms, the partial order and group curriculums, which easily follow the theoretical understanding of the corresponding SPCL regimes.</p>
    </div>
    
        <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@articleliu2018understanding</span><span class="err">,</span>
  <span class="err">title=Understanding</span> <span class="err">self-paced</span> <span class="err">learning</span> <span class="err">under</span> <span class="err">concave</span> <span class="err">conjugacy</span> <span class="err">theory,</span>
  <span class="err">author=Liu,</span> <span class="err">Shiqi</span> <span class="err">and</span> <span class="err">Ma,</span> <span class="err">Zilu</span> <span class="err">and</span> <span class="err">Meng,</span> <span class="err">Deyu</span> <span class="err">and</span> <span class="err">Wang,</span> <span class="err">Kai-Dong</span> <span class="err">and</span> <span class="err">Zhang,</span> <span class="err">Yong,</span>
  <span class="err">journal=Communiations</span> <span class="err">in</span> <span class="err">Information</span> <span class="err">and</span> <span class="err">Systems,</span>
  <span class="err">year=2018,</span>
  <span class="err">publisher=intlpress</span></code></pre></figure>
    </div>
    
  </div>



</div>
</li>
<li><div class="row">
  <div class="col-sm-3 abbr">
  
    <img src="/BLOG/assets/teaser/cover_is_subdiverity.png" class="teaser img-fluid z-depth-1" />
  
  
        
            <abbr class="badge">Information Sciences</abbr>
        
  
  </div>

  <div id="ma2018convergence" class="col-sm-9">
    
      <div class="title">On convergence properties of implicit self-paced objective</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                <a href="https://scholar.google.com/citations?hl=en&amp;user=OHEfQU0AAAAJ" target="_blank">Zilu Ma</a>,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                Shiqi Liu,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                <a href="https://scholar.google.com/citations?hl=en&amp;user=an6w-64AAAAJ" target="_blank">Deyu Meng</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                    Yong Zhang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                    SioLong Lo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                and Zhi Han
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Information Sciences</em>
      
      
        (<b>Information Sciences</b>)
      
      
      , 2018
      
      </div>
    

    <div class="links">
    
      
      <a href="https://arxiv.org/pdf/1703.09923" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
    
    
    
    </div>
        <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Self-paced learning (SPL) is a new methodology that simulates the learning principle of humans/animals to start learning easier aspects of a learning task, and then gradually take more complex examples into training. This new-coming learning regime has been empirically substantiated to be effective in various computer vision and pattern recognition tasks. Recently, it has been proved that the SPL regime has a close relationship to a implicit self-paced objective function. While this implicit objective could provide helpful interpretations to the effectiveness, especially the robustness, insights under the SPL paradigms, there are still no theoretical results strictly proved to verify such relationship. To this issue, in this paper, we provide some convergence results on this implicit objective of SPL. Specifically, we prove that the learning process of SPL always converges to critical points of this implicit objective under some mild conditions. This result verifies the intrinsic relationship between SPL and this implicit objective, and makes the previous robustness analysis on SPL complete and theoretically rational.</p>
    </div>
    
        <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@articleliu2018understanding</span><span class="err">,</span>
  <span class="err">title=On</span> <span class="err">convergence</span> <span class="err">properties</span> <span class="err">of</span> <span class="err">implicit</span> <span class="err">self-paced</span> <span class="err">objective,</span>
  <span class="err">author=Ma,</span> <span class="err">Zilu</span> <span class="err">and</span> <span class="err">Liu,</span> <span class="err">Shiqi</span> <span class="err">and</span> <span class="err">Meng,</span> <span class="err">Deyu</span> <span class="err">and</span> <span class="err">Zhang,</span> <span class="err">Yong</span> <span class="err">and</span> <span class="err">Lo,</span> <span class="err">SioLong</span> <span class="err">and</span> <span class="err">Han,</span> <span class="err">Zhi,</span>
  <span class="err">journal=Information</span> <span class="err">Sciences,</span>
  <span class="err">year=2018,</span>
  <span class="err">publisher=Elsevier</span></code></pre></figure>
    </div>
    
  </div>



</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3 abbr">
  
    <img src="/BLOG/assets/teaser/selfpacedsuvey.jpg" class="teaser img-fluid z-depth-1" />
  
  
        
            <abbr class="badge">Bachelor</abbr>
        
  
  </div>

  <div id="liu2017Survey" class="col-sm-9">
    
      <div class="title">Ëá™Ê≠•Â≠¶‰π†Á†îÁ©∂ÁªºËø∞(A Survey On Self-paced Learning)</div>
      <div class="author">
        
          
          
          
          
            
              
            
          
          
          
            
            Shiqi Liu
            
          
        
      </div>

      <div class="periodical">
      
        <em>Xi‚Äôan jiaotong University Bachelor Thesis</em>
      
      
        (<b>Bachelor</b>)
      
      
      , 2017
      
      </div>
    

    <div class="links">
    
      
      <a href="/BLOG/assets/pdf/bachelor.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
    
    
      
      <a href="/BLOG/assets/pdf/bachelorrepresentation.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    </div>
        <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p> In the era of big data, a lot of information is contained in a large amount of imperfect data. In order to extract useful information from those data, machine learning algorithms should be provided with the abilities of both data selection and data learning. To solve this problem, drawing lessons from the principle of human curriculum learning, which is to learn from simple to difficult adaptively, a new learning regime called self-paced learning(SPL) has appeared in recent years. SPL has developed rapidly and made extensive applications in the fields of machine learning, mainly including semi-supervised learning, weakly supervised learning, robust learning, and very few samples learning. This paper surveys recent achievements and related applications of SPL in machine learning fields. We review the classical curriculum learning and SPL regimes, further, summarize the internal prior knowledge utilized by the SPL and external curriculum prior knowledge utilized by the self-paced curriculum learning, SPL with diversity and self-paced co-training. Current advancements of optimization theory regarding SPL are introduced and united together under the concave conjugacy of SPL. In particular, we first raise the probability viewpoint for SPL, obtain the upper bound of the optimal performance as well as the influence factor of SPL and render a general probability framework for SPL. In the end, we look into the future of the SPL in the closed and open learning environment. The general probability viewpoint could motivate the design of the easiness standard, the formulation of novel data selection principles, the integration with other learning algorithms regarding the self-paced methodologies which would further benefit the promotion and applications of SPL in practical problems.</p>
    </div>
    
        <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@articleliu2017Survey</span><span class="err">,</span>
  <span class="err">title=Ëá™Ê≠•Â≠¶‰π†Á†îÁ©∂ÁªºËø∞</span><span class="p">(</span><span class="nl">A</span> <span class="err">Survey</span> <span class="err">On</span> <span class="err">Self-paced</span> <span class="err">Learning</span><span class="p">)</span><span class="c">,</span>
  <span class="c">author=Liu, Shiqi,</span>
  <span class="c">journal=Xi‚Äôan jiaotong University Bachelor,</span>
  <span class="c">year=2017,</span>
  <span class="c">publisher=</span></code></pre></figure>
    </div>
    
  </div>



</div>
</li>
<li><div class="row">
  <div class="col-sm-3 abbr">
  
    <img src="/BLOG/assets/teaser/mixtureofnoise.jpg" class="teaser img-fluid z-depth-1" />
  
  
        
            <abbr class="badge">Openreview</abbr>
        
  
  </div>

  <div id="liu2017fitting" class="col-sm-9">
    
      <div class="title">Fitting Data Noise in Variational Autoencoder</div>
      <div class="author">
        
          
          
          
          
            
              
            
          
          
          
            
              
                Shiqi Liu,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                <a href="https://scholar.google.com/citations?hl=en&amp;user=vM6yGTEAAAAJ" target="_blank">Qian Zhao</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                <a href="https://scholar.google.com/citations?hl=en&amp;user=IePM9RsAAAAJ" target="_blank">Xiangyong Cao</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                and <a href="https://scholar.google.com/citations?hl=en&amp;user=an6w-64AAAAJ" target="_blank">Deyu Meng</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Openreview</em>
      
      
        (<b>Openreview</b>)
      
      
      , 2017
      
      </div>
    

    <div class="links">
    
      
      <a href="https://openreview.net/references/pdf?id=SJTGmVZ0b" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
      <a href="https://github.com/647LiuSQ/Discovering-influential-factors-in-variational-autoencoders" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>
        <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Why does variational autoencoder(VAE) suffer from bad reconstruction, and what influence the disentanglement of VAE? This paper tries to address those issues through a noise modelling perspective. On one fold, the paper proposes the adaptive noise learning algorithms of Gaussian noise and mixture Gaussian noise assumption which empirically contributes to a better reconstruction than original
VAE noise assumptions. On other fold, several generating factor properties in the idealistic VAE case are discussed and several performance indicators regarding the disentanglement and generating influence are subsequently raised to evaluate the performance of VAE model and to supervise the used factors. Theoretical analysis is reflexed in the experiment results.</p>
    </div>
    
        <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@articleliu2017fitting</span><span class="err">,</span>
  <span class="err">title=Fitting</span> <span class="err">Data</span> <span class="err">Noise</span> <span class="err">in</span> <span class="err">Variational</span> <span class="err">Autoencoder,</span>
  <span class="err">author=Liu,</span> <span class="err">Shiqi</span> <span class="err">and</span> <span class="err">Zhao,</span> <span class="err">Qian,</span> <span class="err">and</span> <span class="err">Cao,</span> <span class="err">Xiangyong</span> <span class="err">and</span> <span class="err">Meng,</span> <span class="err">Deyu,</span>
  <span class="err">journal=Openreview,</span>
  <span class="err">year=2017,</span>
  <span class="err">publisher=Openreview</span></code></pre></figure>
    </div>
    
  </div>



</div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3 abbr">
  
    <img src="/BLOG/assets/teaser/psc.jpg" class="teaser img-fluid z-depth-1" />
  
  
        
            <abbr class="badge">Geography Teaching</abbr>
        
  
  </div>

  <div id="liu2015Vapor" class="col-sm-9">
    
      <div class="title">Ê∞¥Ëí∏Ê±ΩÂØπÊ∞îÂÄôÂèòÊöñÂΩ±ÂìçÁöÑÂÆûÈ™åÊä•Âëä</div>
      <div class="author">
        
      </div>

      <div class="periodical">
      
        <em>Âú∞ÁêÜÊïôÂ≠¶ (Geography Teaching)</em>
      
      
        (<b>Geography Teaching</b>)
      
      
      , 2015
      
      </div>
    

    <div class="links">
    
      
      <a href="http://www.cqvip.com/qk/96717x/201524/667665821.html" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
    
    
    
    </div>
        <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>  ÈÄöËøáÂõõÊ¨°ÂÆöÊÄßÂØπÊØîÂÆûÈ™å,‰∏ÄÊ¨°Ê®°ÊãüÂÖâÊ∫êÂÆöÈáèÂÆûÈ™å,ÂØπÊ∞¥Ëí∏Ê±ΩÁöÑÂê∏ÁÉ≠‰ΩúÁî®,Ê∏©ÂÆ§ÊïàÂ∫îËøõË°å‰∫ÜÁ†îÁ©∂,Áî®ExcelËΩØ‰ª∂Â∞ÜÂÆûÈ™åÊï∞ÊçÆÂà∂‰ΩúÊàêÂÆûÊó∂Ê∏©Â∫¶ÂèòÂåñÊõ≤Á∫øÂõæ,Áõ¥ËßÇÊòæÁ§∫ÂÆûÈ™åÁöÑÁªìÊûú.ÂÆûÈ™åËØÅÊòé:Âú®Áõ∏ÂêåËæêÂ∞ÑÊù°‰ª∂‰∏ãÁöÑÂØÜÈó≠ÂÆπÂô®‰∏≠,Ê∏©Â∫¶ÁöÑ‰∏äÂçá‰∏éÊ∞¥Ëí∏Ê±ΩÂê´ÈáèÊ≠£Áõ∏ÂÖ≥;ÂçáÊ∏©ËøáÁ®ã‰∏≠,Ê∞¥Ëí∏Ê±ΩË∂äÂ§ö,Ë∂äÊôöÂà∞ËææÂπ≥Ë°°Ê∏©Â∫¶,‰∏îÂπ≥Ë°°Ê∏©Â∫¶Ë∂äÈ´ò;‰∏çËÆ∫Âú®ÂçáÊ∏©ËøòÊòØÈôçÊ∏©ËøáÁ®ã‰∏≠,Ê∞¥Ëí∏Ê±ΩË∂äÂ§öÊ∏©Â∫¶Ë∂äÈ´ò.</p>
    </div>
    
        <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@articleliu2015Vapor</span><span class="err">,</span>
  <span class="err">title=Ê∞¥Ëí∏Ê±ΩÂØπÊ∞îÂÄôÂèòÊöñÂΩ±ÂìçÁöÑÂÆûÈ™åÊä•Âëä,</span>
  <span class="err">author=Âàò‰ªïÁê™</span> <span class="p">(</span><span class="nl">Shiqi</span> <span class="err">Liu</span><span class="p">)</span><span class="c">, ÈáëÂπøÁ¶π, ÊΩòÂä≠Âπ≥, È©¨ËÖæ, ÂàòÂÆáÊõº, Âç¢Ê∂µ, Èôà‰ΩëÂÆÅ, ÈôàÂºèÂ¶Ç,</span>
  <span class="c">journal=Âú∞ÁêÜÊïôÂ≠¶ (Geography Teaching),</span>
  <span class="c">year=2015,</span>
  <span class="c">publisher=</span></code></pre></figure>
    </div>
    
  </div>



</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2024 Shiqi  Liu.
    
    
    
    Last updated: January 26, 2024.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/BLOG/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/BLOG/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/BLOG/assets/js/common.js"></script>


</html>
