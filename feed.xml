<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/BLOG/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/BLOG/" rel="alternate" type="text/html" /><updated>2023-02-28T11:42:26+08:00</updated><id>http://localhost:4000/BLOG/feed.xml</id><title type="html">Shiqi Liu</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Understanding Self-Paced Learning</title><link href="http://localhost:4000/BLOG/blog/2023/Understanding-Self-Paced-Learning/" rel="alternate" type="text/html" title="Understanding Self-Paced Learning" /><published>2023-02-27T08:00:00+08:00</published><updated>2023-02-27T08:00:00+08:00</updated><id>http://localhost:4000/BLOG/blog/2023/Understanding-Self-Paced-Learning</id><content type="html" xml:base="http://localhost:4000/BLOG/blog/2023/Understanding-Self-Paced-Learning/"><![CDATA[<p>I have studied the theoretical implications of self-paced learning explanations in the past. More specifically, I have studied the optimization perspective of self-paced learning format and the understanding of self-paced learning probability perspective in the past. My team members and I developed a solid understanding of "what self-paced learning is optimizing?" and "how self-paced learning models learn the data distribution".</p>

<p>We find that the objective function of self-paced learning is closely related to the "approximate inference" process that introduces a latent variable v, the difficulty weight variable. At the same time, v has different physical meanings in different self-paced learning applications. We find that the success of self-paced learning in different applications benefits from different prior knowledge embedded by v with specific application-related physical meanings.</p>

<p>Sincere advice to researchers working on self-paced learning: please explore the physical meaning of v in your application and then try to embed more/more general priors using some suitable/novel methods. From the perspective of robust learning, self-paced learning methodologies are still very promising in various weak and noisy learning scenarios (tracking, weak object detection, object localization, learning webpage tagged data, etc.). However, self-paced learning should not be limited to existing reweighting frameworks, whereas estimation-inference frameworks may be more promising. Some emerging meta-weighted networks may also be a good direction.
</p>]]></content><author><name>Shiqi Liu</name></author><summary type="html"><![CDATA[Some finding from Understanding Self-Paced Learning under Concave Conjugacy Theory paper.]]></summary></entry><entry><title type="html">Information Separation and Sparsity Induced by VAE Object Function</title><link href="http://localhost:4000/BLOG/blog/2023/Information-Separation-and-Spasity-Induced-by-VAE-Object-Function/" rel="alternate" type="text/html" title="Information Separation and Sparsity Induced by VAE Object Function" /><published>2023-02-27T08:00:00+08:00</published><updated>2023-02-27T08:00:00+08:00</updated><id>http://localhost:4000/BLOG/blog/2023/Information-Separation-and-Spasity-Induced-by-VAE-Object-Function</id><content type="html" xml:base="http://localhost:4000/BLOG/blog/2023/Information-Separation-and-Spasity-Induced-by-VAE-Object-Function/"><![CDATA[<p>We believe that data can be generated by a number of independent generators. In the work of discovering the influential generation factors of variational autoencoder, we found that the KL divergence term optimized by variational autoencoder promotes the independence of factors and promotes the sparsity of factor mutual information. The influential generative factors extracted by variational autoencoder can capture the variation of data well.
<h2 id="Separation in Information">Information Separation</h2>

Due to the separable nature of its mutual information, this allows us to use a small number of factor dimensions to achieve a better effect on subsequent classification or reconstruction tasks.</p>

<h2 id="Sparsity in Information">Information Sparsity</h2>
<p><img src="../../../assets/teaser/VAE_sparsity.png" title="Information separation and sparsity" height="300" /></p>]]></content><author><name>Shiqi Liu</name></author><summary type="html"><![CDATA[Some finding from Discovering Influential Factors in VAEs paper.]]></summary></entry><entry><title type="html">Camera Lidar Calibration</title><link href="http://localhost:4000/BLOG/blog/2019/3d-2d_lidar_camera_calibration/" rel="alternate" type="text/html" title="Camera Lidar Calibration" /><published>2019-04-12T08:00:00+08:00</published><updated>2019-04-12T08:00:00+08:00</updated><id>http://localhost:4000/BLOG/blog/2019/3d-2d_lidar_camera_calibration</id><content type="html" xml:base="http://localhost:4000/BLOG/blog/2019/3d-2d_lidar_camera_calibration/"><![CDATA[<p>Alignment of camera and lidar has important implications for computer vision applications. We need to obtain the external parameters from the radar to the camera (that is, how the radar coordinate system is transformed to the camera coordinate system through a rigid body) when the internal parameters of the camera are known, including the rotation matrix R and translation t.</p>
<p>The correspondence between the point cloud in the radar coordinate system and the points on the pixel plane is as follows:</p>
<p><img src="../../../assets/img/Camera lidar calibration1.png" width="600" alt="avatar" /></p>
<p>Assume that the radar coordinate system is the world coordinate system. one point is<br />
$$ (x_w,y_w,z_w)^T. $$<br />
Transformed to the camera coordinate system after R, t, that is<br />
$$ (x_c,y_c,z_c)^T=R(x_w,y_w,z_w)^T+t^T $$<br />
Coordinates projected onto the imaging plane with Z=1<br />
$$ p=(x_1,y_1)^T=(x_c/z_c,y_c/z_c)^T $$<br />
<img src="../../../assets/img/Camera lidar calibration2.png" width="600" alt="avatar" /></p>
<p>Get new coordinates after distortion transformation<br />
$$ (x_D,y_D)^T=\phi_D(x_1,y_1)^T $$<br />
Get the coordinates of the last pixel plane through the internal reference matrix K<br />
$$ (x_p,y_p)^T=(x_D,y_D)^T K^T. $$<br />
We simply say that the entire mapping function above is<br />
$$ (x_p,y_p)^T=f(x_w,y_w,z_w) $$</p>
<p>In order to obtain the external parameters R and t, we use an original calibration method, which we call L2L(3d-2d). We use the mapping relationship from the 3D line to the 2D line to implement: specifically, the relationship that the corresponding pixel points in 2D are on the straight line projected from 3D to 2D. (The 3D line is still a line after being projected to 2D. We use the intersection of two surfaces in 3D to determine the line in 3D. The surface parameters in 3D are calculated by several points in the point cloud on it. We use several pixels in the pixel space to represent the lines in 2D.) We hope that the distance between the corresponding pixels in 2D and the straight line projected from 3D to 2D should be as small as possible, so the calculation is done by the pixel The distance of a point to a 3D line projected onto the pixel plane forms our optimization function. In the case of a given initial value, the objective function obtained by minimizing a series of corresponding line pairs is obtained to obtain optimized external parameters.</p>
<p>Suppose<br />
$$ (x_w^{1},y_w^{1},z_w^{1})^T,(x_w^{2},y_w^{2},z_w^{2})^T $$<br />
is two points on the 3D line, then the line projected by the 3D line in 2D can be represented by the two points projected on it (the default distortion has little effect and will not make the line curved.)<br />
$$ (x_p^{1},y_p^{1})^T=f(x_w^{1},y_w^{1},z_w^{1}),(x_p^{2},y_p^{2})^T=f(x_w^{2},y_w^{2},z_w^{2}). $$<br />
<img src="../../../assets/img/Camera lidar calibration3.png" width="600" alt="avatar" /></p>
<p>Then we can find the equation of the straight line<br />
$$ Ax+By+C=0. $$<br />
For several known pixel points on a straight line<br />
$$ (x_p^a,y_p^a)^T,(x_p^b,y_p^b)^T,\cdots, $$<br />
Its distance to the straight line is<br />
$$ d^a=\vert\frac{Ax_p^a+By_p^a+C}{\sqrt{A^2+B^2}} \vert,\cdots, $$<br />
The objective function of a line is constructed:<br />
$$ L_{line1}=d^a+d^b+\cdots, $$<br />
The objective function for all lines then becomes<br />
$$ totalLoss=L_{line1}+L_{line2}+\cdots. $$</p>
<p>It is noted that the projection to the Z=1 plane is a nonlinear operation during the whole process, which makes the entire loss nonlinear and very complicated with respect to R and t, and cannot be solved directly analytically. We use the global optimization function basinhopping in scipy's optimize in python to optimize the objective function with respect to R, t.</p>
<p>Since bashihoping is a global optimization function, it is greatly affected by the initial value of the search. And R is not well suited for direct optimization because its degrees of freedom are lower than its number of variables. So we use the Euler angle RPY to describe the R matrix (the space from the Euler angle to the R matrix is a surjection. By enumerating the RPY feasible region and taking points as the initial value of the bashihoping function to optimize. We will try 0 and 180Â° as the initial value for each dimension, a total of 8 situations, and empirically, good results can be obtained under the 4 initial values.</p>
<p>We have also tried the method of using point-to-point correspondence to solve the external parameters, that is, replacing the distance from point to line with the distance from point to point, but we found that it is difficult to determine the correspondence between 3D points and 2D points. After verifying that its solution effect is very poor, we finally gave up this method.</p>]]></content><author><name>Shiqi Liu</name></author><summary type="html"><![CDATA[These are tidbits of my internship in Hesaitech.]]></summary></entry><entry><title type="html">GAN and VAE Object Function</title><link href="http://localhost:4000/BLOG/blog/2017/GAN-and-VAE-Object-Function/" rel="alternate" type="text/html" title="GAN and VAE Object Function" /><published>2017-06-11T08:00:00+08:00</published><updated>2017-06-11T08:00:00+08:00</updated><id>http://localhost:4000/BLOG/blog/2017/GAN-and-VAE-Object-Function</id><content type="html" xml:base="http://localhost:4000/BLOG/blog/2017/GAN-and-VAE-Object-Function/"><![CDATA[<p>VAE corresponds to the <em>traditional probability learning philosophy</em> : <strong>Maximum Likelihood Principle</strong> (Corresponding to Empirical KL-divergence) and <strong>Maximum A Posterior Principle</strong> Implemented by Approximate Inference Approaches.</p>
<p>GAN corresponds to the <em>new emerging probability learning philosophy</em> : Minimize a bounch of new <strong>Probability Similarity Metrics</strong> Implemented by Implicit Learning Approaches.</p>
<p>Recently, there are a lot of researches focused on uniting the two novel generative model: GAN and VAE. Those works provides their understanding with several rebuilt models regarding GAN and VAE.</p>
<p>Here we try to understand and memorize those models by their <strong>equivalent object functions</strong> under those probability learning philosophy.</p>
<p>Some common sense for notation: D is the discriminator, G is the Generator, Q is auxiliary encoder, De is the decoder, En is the encoder.</p>
<h1 id="original-gan">Original GAN</h1>
<p><img src="../../../assets/img/OriginalGAN.jpg" width="600" title="This is an example image" /></p>
<h1 id="ipm-gan">IPM GAN</h1>
<p><img src="../../../assets/img/IPMGAN.jpg" width="600" title="This is an example image" /></p>
<h2 id="wgan">WGAN</h2>
<p><img src="../../../assets/img/WGAN.jpg" width="600" title="This is an example image" /></p>
<h2 id="blgan">BLGAN</h2>
<p><img src="../../../assets/img/BLGAN.jpg" width="600" title="This is an example image" /></p>
<h1 id="info-gan">INFO GAN</h1>
<p><img src="../../../assets/img/InfoGAN.jpg" width="600" title="This is an example image" /></p>
<h1 id="ae">AE</h1>
<p><img src="../../../assets/img/AE.jpg" width="600" title="This is an example image" /></p>
<h1 id="vae">VAE</h1>
<p><img src="../../../assets/img/VAE1.jpg" width="600" title="This is an example image" /></p>
<h2 id="condtional-vae">Condtional VAE</h2>
<p><img src="../../../assets/img/CVAE1.jpg" width="600" title="This is an example image" /></p>]]></content><author><name>Shiqi Liu</name></author><summary type="html"><![CDATA[Some finding from VAE Tutorial and GAN papers.]]></summary></entry></feed>